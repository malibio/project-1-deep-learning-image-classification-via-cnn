{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d279c439",
      "metadata": {
        "id": "d279c439"
      },
      "source": [
        "### Import the CNN Util and libraries needed\n",
        "We have the util to make it easy to create and try new variations of the CNN model and be consistent with how we're analyzing and evaluating it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5abe186c",
      "metadata": {
        "id": "5abe186c"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import cnn_utils\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from keras import layers, models, optimizers, callbacks\n",
        "from keras.src.legacy.preprocessing.image import ImageDataGenerator\n",
        "from keras.applications import ResNet50, EfficientNetB0, VGG16\n",
        "from keras.applications.imagenet_utils import preprocess_input\n",
        "from keras.applications import ResNet50V2\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from tensorflow.keras.preprocessing.image import smart_resize\n",
        "from keras.src.legacy.preprocessing.image import ImageDataGenerator\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43599bbb",
      "metadata": {
        "id": "43599bbb"
      },
      "source": [
        "### Load the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a77831e4",
      "metadata": {
        "id": "a77831e4"
      },
      "outputs": [],
      "source": [
        "data_dict = cnn_utils.load_cifar10_from_tar()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05ba80d8",
      "metadata": {
        "id": "05ba80d8"
      },
      "source": [
        "### Preporcess the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a5c3162",
      "metadata": {
        "id": "1a5c3162"
      },
      "outputs": [],
      "source": [
        "data = cnn_utils.preprocess_data(data_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69f8bb96",
      "metadata": {
        "id": "69f8bb96"
      },
      "source": [
        "### Let's do a quick visualization of sample images (to also ensure we still have the correct shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3efbcbda",
      "metadata": {
        "id": "3efbcbda"
      },
      "outputs": [],
      "source": [
        "cnn_utils.visualize_data_samples(data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3bae54d",
      "metadata": {
        "id": "f3bae54d"
      },
      "source": [
        "### Data processing for Transfer Learning\n",
        "- Converts back to [0, 255]: Your data was normalized to [0, 1], but ImageNet pre-trained models expect the original [0, 255] pixel range\n",
        "- Applies ImageNet preprocessing: Uses preprocess_input() which applies model-specific normalization (e.g., ResNet uses different normalization than VGG)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1236c979",
      "metadata": {
        "id": "1236c979"
      },
      "outputs": [],
      "source": [
        "print(\"Resizing only validation and test sets (training will be handled by augmentation)...\")\n",
        "\n",
        "# Only resize val and test sets (much smaller)\n",
        "data['X_val'] = tf.image.resize(data['X_val'], [224, 224]).numpy()\n",
        "data['X_test'] = tf.image.resize(data['X_test'], [224, 224]).numpy()\n",
        "\n",
        "print(\"Val and test resizing completed!\")\n",
        "print(f\"Shapes - Train: {data['X_train'].shape} (32x32), Val: {data['X_val'].shape} (224x224), Test: {data['X_test'].shape} (224x224)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c338efe8",
      "metadata": {
        "id": "c338efe8"
      },
      "source": [
        "### Data Augmentation\n",
        "\n",
        "Moderate geometric augmentation that applies realistic transformations to training images:\n",
        "- Rotation (±15°), shifting (10% in each direction), and zooming (±10%) simulate natural camera angle and distance variations\n",
        "- Horizontal flipping doubles the dataset by creating mirror images (works well for CIFAR-10 since objects like cars/planes look realistic when flipped)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7df6db98",
      "metadata": {
        "id": "7df6db98"
      },
      "outputs": [],
      "source": [
        "def create_augmentation():\n",
        "    def resize_and_augment(x):\n",
        "        # Resize from 32x32 to 224x224 during training\n",
        "        x_resized = tf.image.resize(x, [224, 224])\n",
        "        return x_resized  # Remove .numpy() - keep as tensor\n",
        "\n",
        "    return ImageDataGenerator(\n",
        "        preprocessing_function=resize_and_augment,\n",
        "        rotation_range=10,\n",
        "        width_shift_range=0.05,\n",
        "        height_shift_range=0.05,\n",
        "        horizontal_flip=True,\n",
        "        zoom_range=0.05\n",
        "    )\n",
        "\n",
        "augmentation = create_augmentation()\n",
        "augmentation.fit(data['X_train'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dee7839c",
      "metadata": {
        "id": "dee7839c"
      },
      "source": [
        "### Let's define our CNN model (architecture)\n",
        "Deeper, more sophisticated architecture for higher accuracy\n",
        "Structure:\n",
        "- 3 convolutional blocks (64→128→256 filters)\n",
        "- BatchNormalization after each conv layer\n",
        "- Progressive dropout (0.3→0.4→0.5)\n",
        "- Large dense layer (512 neurons)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e130d358",
      "metadata": {
        "id": "e130d358"
      },
      "source": [
        "### Create transfer model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ab7001b",
      "metadata": {
        "id": "8ab7001b"
      },
      "outputs": [],
      "source": [
        "def create_transfer_model(base_model_name='resnet50', num_classes=10):\n",
        "    \"\"\"\n",
        "    Create transfer learning model with frozen base and custom classifier\n",
        "    \"\"\"\n",
        "    # Choose base model\n",
        "    if base_model_name == 'resnet50':\n",
        "        base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(32, 32, 3))\n",
        "    elif base_model_name == 'efficientnet':\n",
        "        base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(32, 32, 3))\n",
        "    elif base_model_name == 'vgg16':\n",
        "        base_model = VGG16(weights='imagenet', include_top=False, input_shape=(32, 32, 3))\n",
        "    elif base_model_name == 'resnet50v2':\n",
        "        base_model = ResNet50V2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "    # Freeze the base model\n",
        "    base_model.trainable = False\n",
        "\n",
        "    # Build the complete model\n",
        "    model = models.Sequential([\n",
        "        base_model,\n",
        "        layers.GlobalAveragePooling2D(),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(128, activation='relu'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    return model, base_model\n",
        "\n",
        "# Create the model\n",
        "model, base_model = create_transfer_model('resnet50v2')\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "cnn_utils.print_model_summary(model)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "callbacks = [\n",
        "    ReduceLROnPlateau(factor=0.5, patience=5, min_lr=1e-7)\n",
        "]"
      ],
      "metadata": {
        "id": "oyl_GcltBvnj"
      },
      "id": "oyl_GcltBvnj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "e14f7818",
      "metadata": {
        "id": "e14f7818"
      },
      "source": [
        "### Phase 1 Training (Frozen Base)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78074c37",
      "metadata": {
        "id": "78074c37"
      },
      "outputs": [],
      "source": [
        "print(\"=== PHASE 1: Training classifier with frozen base ===\")\n",
        "# Train with frozen base model\n",
        "history_phase1 = cnn_utils.train_model(\n",
        "    model,\n",
        "    data,\n",
        "    augmentation=augmentation,\n",
        "    epochs=20,\n",
        "    batch_size=32,\n",
        "    callbacks=callbacks\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b9f6af9",
      "metadata": {
        "id": "2b9f6af9"
      },
      "source": [
        "### Phase 2 Training (Fine-tuning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4e71b87",
      "metadata": {
        "id": "c4e71b87"
      },
      "outputs": [],
      "source": [
        "print(\"=== PHASE 2A: Partial unfreezing (last 50 layers) ===\")\n",
        "\n",
        "# Unfreeze only the last 50 layers\n",
        "for layer in base_model.layers[:-50]:\n",
        "    layer.trainable = False\n",
        "for layer in base_model.layers[-50:]:\n",
        "    layer.trainable = True\n",
        "\n",
        "# Recompile with higher learning rate for partial fine-tuning\n",
        "model.compile(\n",
        "    optimizer=optimizers.Adam(learning_rate=1e-4),  # Higher than before\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Train with partial unfreezing\n",
        "history_phase2a = cnn_utils.train_model(\n",
        "    model,\n",
        "    data,\n",
        "    augmentation=augmentation,\n",
        "    epochs=10,\n",
        "    batch_size=32,\n",
        "    callbacks=callbacks\n",
        ")\n",
        "\n",
        "print(\"=== PHASE 2B: Full fine-tuning (all layers) ===\")\n",
        "\n",
        "# Unfreeze all layers\n",
        "base_model.trainable = True\n",
        "\n",
        "# Recompile with very low learning rate for full fine-tuning\n",
        "model.compile(\n",
        "    optimizer=optimizers.Adam(learning_rate=1e-5),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Continue training with all layers unfrozen\n",
        "history_phase2b = cnn_utils.train_model(\n",
        "    model,\n",
        "    data,\n",
        "    augmentation=augmentation,\n",
        "    epochs=10,\n",
        "    batch_size=32,\n",
        "    callbacks=callbacks\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b49b6f7",
      "metadata": {
        "id": "5b49b6f7"
      },
      "source": [
        "### Combined History"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60008633",
      "metadata": {
        "id": "60008633"
      },
      "outputs": [],
      "source": [
        "# Combine all three training phases\n",
        "def combine_three_histories(hist1, hist2a, hist2b):\n",
        "    combined = {}\n",
        "    for key in hist1.history.keys():\n",
        "        combined[key] = hist1.history[key] + hist2a.history[key] + hist2b.history[key]\n",
        "\n",
        "    class CombinedHistory:\n",
        "        def __init__(self, history_dict):\n",
        "            self.history = history_dict\n",
        "\n",
        "    return CombinedHistory(combined)\n",
        "\n",
        "# Combine all training phases\n",
        "combined_history = combine_three_histories(history_phase1, history_phase2a, history_phase2b)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01069617",
      "metadata": {
        "id": "01069617"
      },
      "source": [
        "### Let's show the evaluation result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34a2a80e",
      "metadata": {
        "id": "34a2a80e"
      },
      "outputs": [],
      "source": [
        "cnn_utils.evaluate_model(model, data, combined_history)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}